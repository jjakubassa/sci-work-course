@article{alahi2016,
  title = {Social {{LSTM}}: {{Human Trajectory Prediction}} in {{Crowded Spaces}}},
  shorttitle = {Social {{LSTM}}},
  author = {Alahi, Alexandre and Goel, Kratarth and Ramanathan, Vignesh and Robicquet, Alexandre and {Fei-Fei}, Li and Savarese, Silvio},
  year = {2016},
  month = jun,
  journal = {2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
  pages = {961--971},
  publisher = {{IEEE}},
  address = {{Las Vegas, NV, USA}},
  doi = {10.1109/CVPR.2016.110},
  urldate = {2024-01-20},
  abstract = {Pedestrians follow different trajectories to avoid obstacles and accommodate fellow pedestrians. Any autonomous vehicle navigating such a scene should be able to foresee the future positions of pedestrians and accordingly adjust its path to avoid collisions. This problem of trajectory prediction can be viewed as a sequence generation task, where we are interested in predicting the future trajectory of people based on their past positions. Following the recent success of Recurrent Neural Network (RNN) models for sequence prediction tasks, we propose an LSTM model which can learn general human movement and predict their future trajectories. This is in contrast to traditional approaches which use hand-crafted functions such as Social forces. We demonstrate the performance of our method on several public datasets. Our model outperforms state-of-the-art methods on some of these datasets. We also analyze the trajectories predicted by our model to demonstrate the motion behaviour learned by our model.},
  isbn = {9781467388511}
}

@article{azar2021,
  title = {Drone {{Deep Reinforcement Learning}}: {{A Review}}},
  shorttitle = {Drone {{Deep Reinforcement Learning}}},
  author = {Azar, Ahmad Taher and Koubaa, Anis and Ali Mohamed, Nada and Ibrahim, Habiba A. and Ibrahim, Zahra Fathy and Kazim, Muhammad and Ammar, Adel and Benjdira, Bilel and Khamis, Alaa M. and Hameed, Ibrahim A. and Casalino, Gabriella},
  year = {2021},
  month = jan,
  journal = {Electronics},
  volume = {10},
  number = {9},
  pages = {999},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {2079-9292},
  doi = {10.3390/electronics10090999},
  urldate = {2024-01-20},
  abstract = {Unmanned Aerial Vehicles (UAVs) are increasingly being used in many challenging and diversified applications. These applications belong to the civilian and the military fields. To name a few; infrastructure inspection, traffic patrolling, remote sensing, mapping, surveillance, rescuing humans and animals, environment monitoring, and Intelligence, Surveillance, Target Acquisition, and Reconnaissance (ISTAR) operations. However, the use of UAVs in these applications needs a substantial level of autonomy. In other words, UAVs should have the ability to accomplish planned missions in unexpected situations without requiring human intervention. To ensure this level of autonomy, many artificial intelligence algorithms were designed. These algorithms targeted the guidance, navigation, and control (GNC) of UAVs. In this paper, we described the state of the art of one subset of these algorithms: the deep reinforcement learning (DRL) techniques. We made a detailed description of them, and we deduced the current limitations in this area. We noted that most of these DRL methods were designed to ensure stable and smooth UAV navigation by training computer-simulated environments. We realized that further research efforts are needed to address the challenges that restrain their deployment in real-life scenarios.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/3TQCAPBL/Azar et al. - 2021 - Drone Deep Reinforcement Learning A Review.pdf}
}

@misc{ball2023,
  title = {Efficient {{Online Reinforcement Learning}} with {{Offline Data}}},
  author = {Ball, Philip J. and Smith, Laura and Kostrikov, Ilya and Levine, Sergey},
  year = {2023},
  month = may,
  number = {arXiv:2302.02948},
  eprint = {2302.02948},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-17},
  abstract = {Sample efficiency and exploration remain major challenges in online reinforcement learning (RL). A powerful approach that can be applied to address these issues is the inclusion of offline data, such as prior trajectories from a human expert or a sub-optimal exploration policy. Previous methods have relied on extensive modifications and additional complexity to ensure the effective use of this data. Instead, we ask: can we simply apply existing off-policy methods to leverage offline data when learning online? In this work, we demonstrate that the answer is yes; however, a set of minimal but important changes to existing off-policy RL algorithms are required to achieve reliable performance. We extensively ablate these design choices, demonstrating the key factors that most affect performance, and arrive at a set of recommendations that practitioners can readily apply, whether their data comprise a small number of expert demonstrations or large volumes of sub-optimal trajectories. We see that correct application of these simple recommendations can provide a \${\textbackslash}mathbf\{2.5{\textbackslash}times\}\$ improvement over existing approaches across a diverse set of competitive benchmarks, with no additional computational overhead. We have released our code at https://github.com/ikostrikov/rlpd.},
  archiveprefix = {arxiv},
  file = {/Users/jonas/Zotero/storage/HZIUH4VV/Ball et al. - 2023 - Efficient Online Reinforcement Learning with Offli.pdf}
}

@misc{chen2018,
  title = {Socially {{Aware Motion Planning}} with {{Deep Reinforcement Learning}}},
  author = {Chen, Yu Fan and Everett, Michael and Liu, Miao and How, Jonathan P.},
  year = {2018},
  month = may,
  number = {arXiv:1703.08862},
  eprint = {1703.08862},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1703.08862},
  urldate = {2023-09-15},
  abstract = {For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules (e.g., passing on the right). However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.},
  archiveprefix = {arxiv}
}

@misc{chen2018a,
  title = {Socially {{Aware Motion Planning}} with {{Deep Reinforcement Learning}}},
  author = {Chen, Yu Fan and Everett, Michael and Liu, Miao and How, Jonathan P.},
  year = {2018},
  month = may,
  number = {arXiv:1703.08862},
  eprint = {1703.08862},
  primaryclass = {cs},
  publisher = {{arXiv}},
  urldate = {2024-01-20},
  abstract = {For robotic vehicles to navigate safely and efficiently in pedestrian-rich environments, it is important to model subtle human behaviors and navigation rules (e.g., passing on the right). However, while instinctive to humans, socially compliant navigation is still difficult to quantify due to the stochasticity in people's behaviors. Existing works are mostly focused on using feature-matching techniques to describe and imitate human paths, but often do not generalize well since the feature values can vary from person to person, and even run to run. This work notes that while it is challenging to directly specify the details of what to do (precise mechanisms of human navigation), it is straightforward to specify what not to do (violations of social norms). Specifically, using deep reinforcement learning, this work develops a time-efficient navigation policy that respects common social norms. The proposed method is shown to enable fully autonomous navigation of a robotic vehicle moving at human walking speed in an environment with many pedestrians.},
  archiveprefix = {arxiv},
  file = {/Users/jonas/Zotero/storage/QLR8VDJP/Chen et al. - 2018 - Socially Aware Motion Planning with Deep Reinforce.pdf}
}

@inproceedings{chen2019a,
  title = {Crowd-{{Robot Interaction}}: {{Crowd-Aware Robot Navigation With Attention-Based Deep Reinforcement Learning}}},
  shorttitle = {Crowd-{{Robot Interaction}}},
  booktitle = {2019 {{International Conference}} on {{Robotics}} and {{Automation}} ({{ICRA}})},
  author = {Chen, Changan and Liu, Yuejiang and Kreiss, Sven and Alahi, Alexandre},
  year = {2019},
  month = may,
  pages = {6015--6022},
  publisher = {{IEEE}},
  address = {{Montreal, QC, Canada}},
  doi = {10.1109/ICRA.2019.8794134},
  urldate = {2024-01-22},
  abstract = {Mobility in an effective and socially-compliant manner is an essential yet challenging task for robots operating in crowded spaces. Recent works have shown the power of deep reinforcement learning techniques to learn socially cooperative policies. However, their cooperation ability deteriorates as the crowd grows since they typically relax the problem as a one-way Human-Robot interaction problem. In this work, we want to go beyond first-order Human-Robot interaction and more explicitly model Crowd-Robot Interaction (CRI). We propose to (i) rethink pairwise interactions with a selfattention mechanism, and (ii) jointly model Human-Robot as well as Human-Human interactions in the deep reinforcement learning framework. Our model captures the Human-Human interactions occurring in dense crowds that indirectly affects the robot's anticipation capability. Our proposed attentive pooling mechanism learns the collective importance of neighboring humans with respect to their future states. Various experiments demonstrate that our model can anticipate human dynamics and navigate in crowds with time efficiency, outperforming state-of-the-art methods.},
  isbn = {978-1-5386-6027-0},
  langid = {english},
  keywords = {checked,navigation,reinforcement learning,robot},
  file = {/Users/jonas/Zotero/storage/UB579QG4/Chen et al. - 2019 - Crowd-Robot Interaction Crowd-Aware Robot Navigat.pdf}
}

@article{cheng2023a,
  title = {Multi-Objective Deep Reinforcement Learning for Crowd-Aware Robot Navigation with Dynamic Human Preference},
  author = {Cheng, Guangran and Wang, Yuanda and Dong, Lu and Cai, Wenzhe and Sun, Changyin},
  year = {2023},
  month = aug,
  journal = {Neural Computing and Applications},
  volume = {35},
  number = {22},
  pages = {16247--16265},
  issn = {0941-0643, 1433-3058},
  doi = {10.1007/s00521-023-08385-4},
  urldate = {2024-01-20},
  abstract = {The growing development of autonomous systems is driving the application of mobile robots in crowded environments. These scenarios often require robots to satisfy multiple conflicting objectives with different relative preferences, such as work efficiency, safety, and smoothness, which inherently cause robots' poor exploration in seeking policies optimizing several performance criteria. In this paper, we propose a multi-objective deep reinforcement learning framework for crowd-aware robot navigation problems to learn policies over multiple competing objectives whose relative importance preference is dynamic to the robot. First, a two-stream structure is introduced to separately extract the spatial and temporal features of pedestrian motion characteristics. Second, to learn navigation policies for each possible preference, a multi-objective deep reinforcement learning method is proposed to maximize a weighted-sum scalarization of different objective functions. We consider path planning and path tracking tasks, which focus on conflicting objectives of collision avoidance, target reaching, and path following. Experimental results demonstrate that our method can effectively navigate through crowds in simulated environments while satisfying different task requirements.},
  langid = {english}
}

@article{cui2021,
  title = {Learning {{World Transition Model}} for {{Socially Aware Robot Navigation}}},
  author = {Cui, Yuxiang and Zhang, Haodong and Wang, Yue and Xiong, Rong},
  year = {2021},
  month = may,
  journal = {2021 IEEE International Conference on Robotics and Automation (ICRA)},
  pages = {9262--9268},
  publisher = {{IEEE}},
  address = {{Xi'an, China}},
  doi = {10.1109/ICRA48506.2021.9561973},
  urldate = {2024-01-20},
  abstract = {Moving in dynamic pedestrian environments is one of the important requirements for autonomous mobile robots. We present a model-based reinforcement learning approach for robots to navigate through crowded environments. The navigation policy is trained with both real interaction data from multi-agent simulation and virtual data from a deep transition model that predicts the evolution of surrounding dynamics of mobile robots. A reward function considering social conventions is designed to guide the training of the policy. Specifically, the policy model takes laser scan sequence and robot's own state as input and outputs steering command. The laser sequence is further transformed into stacked local obstacle maps disentangled from robot's ego motion to separate the static and dynamic obstacles, simplifying the model training. We observe that the policy using our method can be trained with significantly less real interaction data in simulator but achieve similar level of success rate in social navigation tasks compared with other methods. Experiments are conducted in multiple social scenarios both in simulation and on real robots, the learned policy can guide the robots to the final targets successfully in a socially compliant manner. Code is available at https://github.com/YuxiangCui/model-based-social-navigation.},
  isbn = {9781728190778},
  file = {/Users/jonas/Zotero/storage/WQ8UYIJK/Cui et al. - 2021 - Learning World Transition Model for Socially Aware.pdf}
}

@article{esser2023,
  title = {Guided {{Reinforcement Learning}}: {{A Review}} and {{Evaluation}} for {{Efficient}} and {{Effective Real-World Robotics}} [{{Survey}}]},
  shorttitle = {Guided {{Reinforcement Learning}}},
  author = {E{\ss}er, Julian and Bach, Nicolas and Jestel, Christian and Urbann, Oliver and Kerner, S{\"o}ren},
  year = {2023},
  month = jun,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {30},
  number = {2},
  pages = {67--85},
  issn = {1070-9932, 1558-223X},
  doi = {10.1109/MRA.2022.3207664},
  urldate = {2024-01-20},
  abstract = {LEARNING Abstract learning describes the selection of a task-specific action space for a robotics problem that potentially can be hybridized with model-based ap\-p\- roaches.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/IEDKKH6D/Eßer et al. - 2023 - Guided Reinforcement Learning A Review and Evalua.pdf}
}

@article{feng2023,
  title = {Socially {{Aware Hybrid Robot Navigation}} via {{Deep Reinforcement Learning}}},
  author = {Feng, Zhen and Gao, Ming and Xue, Bingxin and Wang, Chaoqun and Zhou, Fengyu},
  year = {2023},
  month = jul,
  journal = {2023 42nd Chinese Control Conference (CCC)},
  pages = {3697--3701},
  publisher = {{IEEE}},
  address = {{Tianjin, China}},
  doi = {10.23919/CCC58697.2023.10240813},
  urldate = {2024-01-20},
  abstract = {Service robots working in public environments require the capacity to navigate among humans and other obstacles safely and socially compliantly. This paper presents a hybrid navigation approach combining rule-based trajectory generators into deep reinforcement learning for motion planning in populated and cluttered environments. An intention-based action space is proposed in the reinforcement learning framework to achieve a tight coupling of the two methods. By fusing static information and dynamic objects, our network can learn motion patterns adapted to real-world scenarios. The rule-based trajectory generator guarantees the safety and dynamic feasibility of the motion primitives. The robot is trained to understand real-time human-robot interactions through deep reinforcement learning. Experiment results demonstrate that our policy can efficiently perceive human interactions and navigate the robot safely in crowded environments with static obstacles.},
  isbn = {9789887581543}
}

@article{fox1997,
  title = {The Dynamic Window Approach to Collision Avoidance},
  author = {Fox, D. and Burgard, W. and Thrun, S.},
  year = {1997},
  month = mar,
  journal = {IEEE Robotics \& Automation Magazine},
  volume = {4},
  number = {1},
  pages = {23--33},
  issn = {10709932},
  doi = {10.1109/100.580977},
  urldate = {2024-01-20},
  abstract = {This approach, designed for mobile robots equipped with synchro-drives, is derived directly from the motion dynamics of the robot. In experiments, the dynamic window approach safely controlled the mobile robot RHINO at speeds of up to 95 cm/sec, in populated and dynamic environments.}
}

@article{he2022,
  title = {Multi-Robot {{Social-aware Cooperative Planning}} in {{Pedestrian Environments Using Multi-agent Reinforcement Learning}}},
  author = {He, Zichen and Song, Chunwei and Dong, Lu},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2211.15901},
  urldate = {2024-01-20},
  abstract = {Safe and efficient co-planning of multiple robots in pedestrian participation environments is promising for applications. In this work, a novel multi-robot social-aware efficient cooperative planner that on the basis of off-policy multi-agent reinforcement learning (MARL) under partial dimension-varying observation and imperfect perception conditions is proposed. We adopt temporal-spatial graph (TSG)-based social encoder to better extract the importance of social relation between each robot and the pedestrians in its field of view (FOV). Also, we introduce K-step lookahead reward setting in multi-robot RL framework to avoid aggressive, intrusive, short-sighted, and unnatural motion decisions generated by robots. Moreover, we improve the traditional centralized critic network with multi-head global attention module to better aggregates local observation information among different robots to guide the process of individual policy update. Finally, multi-group experimental results verify the effectiveness of the proposed cooperative motion planner.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  file = {/Users/jonas/Zotero/storage/YA6NGL8S/He et al. - 2022 - Multi-robot Social-aware Cooperative Planning in P.pdf}
}

@article{helbing1995,
  title = {Social Force Model for Pedestrian Dynamics},
  author = {Helbing, Dirk and Moln{\'a}r, P{\'e}ter},
  year = {1995},
  month = may,
  journal = {Physical Review E},
  volume = {51},
  number = {5},
  pages = {4282--4286},
  issn = {1063-651X, 1095-3787},
  doi = {10.1103/PhysRevE.51.4282},
  urldate = {2024-01-20},
  abstract = {It is suggested that the motion of pedestrians can be described as if they would be subject to ``social forces.'' These ``forces'' are not directly exerted by the pedestrians' personal environment, but they are a measure for the internal motivations of the individuals to perform certain actions (movements). The corresponding force concept is discussed in more detail and can also be applied to the description of other behaviors. In the presented model of pedestrian behavior several force terms are essential: first, a term describing the acceleration towards the desired velocity of motion; second, terms reflecting that a pedestrian keeps a certain distance from other pedestrians and borders; and third, a term modeling attractive effects. The resulting equations of motion of nonlinearly coupled Langevin equations. Computer simulations of crowds of interacting pedestrians show that the social force model is capable of describing the self-organization of several observed collective effects of pedestrian behavior very realistically.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/H7YV32V2/Helbing und Molnár - 1995 - Social force model for pedestrian dynamics.pdf}
}

@article{hundt2020,
  title = {``{{Good Robot}}!'': {{Efficient Reinforcement Learning}} for {{Multi-Step Visual Tasks}} with {{Sim}} to {{Real Transfer}}},
  shorttitle = {``{{Good Robot}}!''},
  author = {Hundt, Andrew and Killeen, Benjamin and Greene, Nicholas and Wu, Hongtao and Kwon, Heeyeon and Paxton, Chris and Hager, Gregory D.},
  year = {2020},
  month = oct,
  journal = {IEEE Robotics and Automation Letters},
  volume = {5},
  number = {4},
  pages = {6724--6731},
  issn = {2377-3766},
  doi = {10.1109/LRA.2020.3015448},
  urldate = {2024-01-20},
  abstract = {Current Reinforcement Learning (RL) algorithms struggle with long-horizon tasks where time can be wasted exploring dead ends and task progress may be easily reversed. We develop the SPOT framework, which explores within action safety zones, learns about unsafe regions without exploring them, and prioritizes experiences that reverse earlier progress to learn with remarkable efficiency. The SPOT framework successfully completes simulated trials of a variety of tasks, improving a baseline trial success rate from 13\% to 100\% when stacking 4 cubes, from 13\% to 99\% when creating rows of 4 cubes, and from 84\% to 95\% when clearing toys arranged in adversarial patterns. Efficiency with respect to actions per trial typically improves by 30\% or more, while training takes just 1-20 k actions, depending on the task. Furthermore, we demonstrate direct sim to real transfer. We are able to create real stacks in 100\% of trials with 61\% efficiency and real rows in 100\% of trials with 59\% efficiency by directly loading the simulation-trained model on the real robot with no additional real-world fine-tuning. To our knowledge, this is the first instance of reinforcement learning with successful sim to real transfer applied to long term multi-step tasks such as block-stacking and row-making with consideration of progress reversal. Code is available at https://github.com/jhu-lcsr/good\_robot.},
  file = {/Users/jonas/Zotero/storage/ZGNUE2CK/Hundt et al. - 2020 - “Good Robot!” Efficient Reinforcement Learning fo.pdf}
}

@article{kastner2022,
  title = {Human-{{Following}} and -Guiding in {{Crowded Environments}} Using {{Semantic Deep-Reinforcement-Learning}} for {{Mobile Service Robots}}},
  author = {K{\"a}stner, Linh and Fatloun, Bassel and Shen, Zhengcheng and Gawrisch, Daniel and Lambrecht, Jens},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2206.05771},
  urldate = {2024-01-20},
  abstract = {Assistance robots have gained widespread attention in various industries such as logistics and human assistance. The tasks of guiding or following a human in a crowded environment such as airports or train stations to carry weight or goods is still an open problem. In these use cases, the robot is not only required to intelligently interact with humans, but also to navigate safely among crowds. Thus, especially highly dynamic environments pose a grand challenge due to the volatile behavior patterns and unpredictable movements of humans. In this paper, we propose a Deep-Reinforcement-Learning-based agent for human-guiding and -following tasks in crowded environments. Therefore, we incorporate semantic information to provide the agent with high-level information like the social states of humans, safety models, and class types. We evaluate our proposed approach against a benchmark approach without semantic information and demonstrated enhanced navigational safety and robustness. Moreover, we demonstrate that the agent could learn to adapt its behavior to humans, which improves the human-robot interaction significantly.},
  copyright = {Creative Commons Attribution 4.0 International}
}

@article{khetarpal2022,
  title = {Towards {{Continual Reinforcement Learning}}: {{A Review}} and {{Perspectives}}},
  shorttitle = {Towards {{Continual Reinforcement Learning}}},
  author = {Khetarpal, Khimya and Riemer, Matthew and Rish, Irina and Precup, Doina},
  year = {2022},
  month = dec,
  journal = {Journal of Artificial Intelligence Research},
  volume = {75},
  pages = {1401--1476},
  issn = {1076-9757},
  doi = {10.1613/jair.1.13673},
  urldate = {2024-01-20},
  abstract = {In this article, we aim to provide a literature review of different formulations and approaches to continual reinforcement learning (RL), also known as lifelong or non-stationary RL. We begin by discussing our perspective on why RL is a natural fit for studying continual learning. We then provide a taxonomy of different continual RL formulations by mathematically characterizing two key properties of non-stationarity, namely, the scope and driver non-stationarity. This offers a unified view of various formulations. Next, we review and present a taxonomy of continual RL approaches. We go on to discuss evaluation of continual RL agents, providing an overview of benchmarks used in the literature and important metrics for understanding agent performance. Finally, we highlight open problems and challenges in bridging the gap between the current state of continual RL and findings in neuroscience. While still in its early days, the study of continual RL has the promise to develop better incremental reinforcement learners that can function in increasingly realistic applications where non-stationarity plays a vital role. These include applications such as those in the fields of healthcare, education, logistics, and robotics.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/X5LBY9GL/Khetarpal et al. - 2022 - Towards Continual Reinforcement Learning A Review.pdf}
}

@article{kim2022,
  title = {Group {{Estimation}} for {{Social Robot Navigation}} in {{Crowded Environments}}},
  author = {Kim, Mincheul and Kwon, Youngsun and Yoon, Sung-Eui},
  year = {2022},
  month = nov,
  journal = {2022 22nd International Conference on Control, Automation and Systems (ICCAS)},
  pages = {1421--1425},
  publisher = {{IEEE}},
  address = {{Jeju, Korea, Republic of}},
  doi = {10.23919/ICCAS55662.2022.10003761},
  urldate = {2024-01-20},
  abstract = {Socially acceptable navigation in a crowded environment is a challenging problem in robotics due to diverse and unknown human intent. Previous studies have dealt with the social navigation problem in dense crowds via multi-robot collision avoidance. However, it is intractable to follow social compliant trajectory since human-robot interaction differs from the multi-robot collision avoidance problem. To approach our goal, this work exploits a human behavior model and focuses on social group actions such as walking together. We observed that human recognizes the other human groups and avoids them during navigation while maintaining social distances. Based on this observation, this paper proposes a social robot navigation method under group space estimation of crowds on a deep reinforcement learning framework. The proposed method estimates the social groups of crowds based on the behavioral similarities in sensory information. Our reinforcement learning framework learns a socially compliant and effective navigation policy through the proposed human group-aware reward. Our experiment in a crowd simulation demonstrates that the proposed approach generates a human-friendly trajectory with improved navigation performance.},
  isbn = {9788993215243}
}

@article{li2023,
  title = {Robot-{{Crowd Navigation}} with {{Socially-Aware Reinforcement Learning Over Graphs}}},
  author = {Li, Benfan and Sun, Jian and Li, Zhuo and Wang, Gang},
  year = {2023},
  month = jul,
  journal = {2023 42nd Chinese Control Conference (CCC)},
  pages = {4286--4291},
  publisher = {{IEEE}},
  address = {{Tianjin, China}},
  doi = {10.23919/CCC58697.2023.10240400},
  urldate = {2024-01-20},
  abstract = {Robots typically perform navigation task in a crowd environment, where the navigation task requires robots to reach a target point safely and efficiently, and to have the least impact on crowd trajectories. To this end, we propose a graph-based socially aware reinforcement learning navigation algorithm, in which the robot-crowd interactions are modeled as a directed spatio-temporal graph. We utilize graph convolutional networks, attention mechanism and long short term memory networks to encode robot-crowd interaction features, which are subsequently leveraged for state value estimation and robot action selection. Our method is demonstrated to have high success rate and short navigation time in various environments and outperform existing methods in terms of security and efficiency.},
  isbn = {9789887581543}
}

@article{liu2023,
  title = {Graph {{Relational Reinforcement Learning}} for {{Mobile Robot Navigation}} in {{Large-Scale Crowded Environments}}},
  author = {Liu, Zhe and Zhai, Yu and Li, Jiaming and Wang, Guangming and Miao, Yanzi and Wang, Hesheng},
  year = {2023},
  month = aug,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {24},
  number = {8},
  pages = {8776--8787},
  issn = {1524-9050, 1558-0016},
  doi = {10.1109/TITS.2023.3269533},
  urldate = {2024-01-20},
  abstract = {Mobile robot autonomous navigation in large-scale environments with crowded dynamic objects and static obstacles is still an essential yet challenging task. Recent works have demonstrated the potential of using deep reinforcement learning to enable autonomous navigation in crowds. However, only considering the human-robot interactions results in short-sighted and unsafe behaviors, and they typically use hand-crafted features and assume the global observation range, leading to large performance declines in large-scale crowded environments. Recent advances have shown the power of graph neural networks to learn local interactions among surrounding objects. In this paper, we consider autonomous navigation task in large-scale environments with crowded static and dynamic objects (such as humans). Particularly, local interactions among dynamic objects are learned for better-understanding their moving tendency and relational graph learning is introduced for aggregating both the object-object interactions and object-robot interactions. In addition, local observations are transformed into graphical inputs to achieve the scalability to various number of surrounding dynamic objects and various static obstacle patterns, and the globally guided reinforcement learning strategy is introduced to achieve the fixed-sized learning model even in large-scale complex environments. Simulation results validate our generalizability to various environments and advanced performance compared with existing works in large-scale crowded environments. In particular, our method with only local observations performs better than the benchmarks with global complete observability. Finally, physical robotic experiments demonstrate our effectiveness and practical applicability in real scenarios.}
}

@article{lu2022,
  title = {Socially Aware Robot Navigation in Crowds via Deep Reinforcement Learning with Resilient Reward Functions},
  author = {Lu, Xiaojun and Woo, Hanwool and Faragasso, Angela and Yamashita, Atsushi and Asama, Hajime},
  year = {2022},
  month = apr,
  journal = {Advanced Robotics},
  volume = {36},
  number = {8},
  pages = {388--403},
  issn = {0169-1864, 1568-5535},
  doi = {10.1080/01691864.2022.2043184},
  urldate = {2024-01-20},
  abstract = {Robots navigating in a robot{\textendash}human coexisting environment need to optimize their paths not only for task-related performance (e.g. safety and efficiency) but also for their social compliance to other pedestrians. This is a crucial yet challenging task. Previous work has shown the power of deep reinforcement learning (DRL) techniques by employing them to train efficient policies for robot navigation. However, their performance deteriorates when the crowd size grows. We cope with this problem by allowing the robot to keep an adapting distance from the pedestrians and perform safe navigation even in high density environments. We first derive a quantitative formula representing the relationship between uncomfortable distance and pedestrian density from a real-word tracking dataset. Then this formula is applied in reward shaping of DRL to get resilient reward functions (R2F). Qualitative and quantitative evaluation results demonstrate that our method outperforms state-of-the-art methods in both low and high pedestrian density environments. GRAPHICAL ABSTRACT},
  langid = {english}
}

@article{lu2023,
  title = {All {{Aware Robot Navigation}} in {{Human Environments Using Deep Reinforcement Learning}}},
  author = {Lu, Xiaojun and Faragasso, Angela and Yamashita, Atsushi and Asama, Hajime},
  year = {2023},
  month = oct,
  journal = {2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages = {5989--5996},
  publisher = {{IEEE}},
  address = {{Detroit, MI, USA}},
  doi = {10.1109/IROS55552.2023.10341477},
  urldate = {2024-01-20},
  abstract = {Mobile robots functioning in human environments should behave with a secure and socially-compliant manner. Although many studies have revealed the effectiveness of Deep Reinforcement Learning (DRL) in robot navigation, most of them can only handle the presence of human as independent individuals. Failing to consider groups may lead to the robot getting stuck or behaving rudely, and omitting to separately handle obstacles from pedestrians will cause low efficiency. In this work, we present a novel all-aware neural network that utilizes DRL to process groups, obstacles, and individuals simultaneously. The proposed solution employs a new Group{\textendash}Robot Interaction (GRI) subnetwork to encode the mutual effects between groups and the robot, and a modified Obstacle{\textendash}Robot Unilateral interaction (ORU) subnetwork is presented to avoid obstacle collisions caused by sensing noises or motion uncertainties. In addition, the influences of a pedestrian, obstacle, and group on other pedestrians or groups, that indirectly affect the robot, are also integrated into the Human{\textendash}Robot Interaction (HRI) subnetwork or GRI subnetwork respectively by using map tensors. Finally, the GRI, ORU, and HRI subnetworks are aggregated into a planning subnetwork to train and derive an all-aware robot navigation policy based on DRL. Evaluation results in both real-world and simulation experiments show that the proposed approach outperforms the current cutting-edge methods.},
  isbn = {9781665491907}
}

@article{ma2022,
  title = {Asymmetric {{Self-Play}} for {{Learning Robust Human-Robot Interaction}} on {{Crowd Navigation Tasks}}},
  author = {Ma, Jun and Qiu, Quecheng and Chen, Shuyan and Yao, Shunyi and Chen, Yuan and Ji, Jianmin},
  year = {2022},
  month = dec,
  journal = {2022 3rd International Conference on Electronics, Communications and Information Technology (CECIT)},
  pages = {211--218},
  publisher = {{IEEE}},
  address = {{Sanya, China}},
  doi = {10.1109/CECIT58139.2022.00045},
  urldate = {2024-01-20},
  abstract = {A robot can make a sound to aware nearby pedestrians during its navigation, which often results in a more efficient and safer trajectory in a crowded environment. However, it is challenging to integrate such interaction capability into an existing robot navigation policy. In this paper, we propose a deep reinforcement learning (DRL) approach for integration, which results in effective interactive robot navigation policies in crowded environments. In particular, we specify the interaction capability by a set of high-level actions with flexible control. Then the navigation policy can be considered as a specific implementation of the `move' action. Based on these high-level actions, we can train a robust human-robot interaction policy via asymmetric self-play, where the robot and some pedestrians, considered as naughty kids, play a game. Then the interaction policy can not only interact with pedestrians who cooperate with the robot but also with pedestrians who try to block the robot. We evaluate our approach in various simulation environments and compare it with multiple approaches. The experimental results show that our approach is robust and performs well in dense environments that are challenging for others. We also deploy the trained policy on a robot and evaluate its performance in multiple real-world crowded environments. A demonstration video is available online at https://youtu.be/x32YmivsIh0.},
  isbn = {9798350331974}
}

@article{moussaoui2023,
  title = {Reinforcement {{Learning}}: {{A}} Review},
  shorttitle = {Reinforcement {{Learning}}},
  author = {Moussaoui, Hanae and El Akkad, Nabil and Benslimane, Mohamed},
  year = {2023},
  month = may,
  journal = {International Journal of Computing and Digital Systems},
  volume = {13},
  number = {1},
  pages = {1465--1483},
  issn = {2210-142X},
  doi = {10.12785/ijcds/1301118},
  urldate = {2024-01-20},
  abstract = {Reinforcement learning is considered a sort of machine learning that acquires knowledge of solving problems using the trial-and-error technique. The process starts with the main actor that is the agent interacting with a given environment and attempting to achieve a multi-step goal within this environment. The environment is characterized by a state that the agent detects and examines. On the other hand, due to the agent's several actions, the environment's state changes according to these modifications. Eventually, and at this stage, the agent gets reward signals as it proceeds nearer to its goal. The agent uses these rewards signals to determine which actions were successful and which actions were not. The state action is then repeated and the reward is looped until the agent learns how to operate effectively within the environment using the trial-and-error concept. The agent's main objective is to learn how to always choose the right action given any state of the environment that leads it closer to its goal. In this paper, we gathered all the methods used in the literature. Multi-armed bandits, the Markov decision process, Monte Carlo methods, dynamic programming as well as temporal-difference learning are some of the corresponding methods used to solve reinforcement learning issues. The current paper is organized and structured as follows: we'll start with an introduction followed by a reinforcement learning section where we discussed all the methods and techniques used in the literature. Furthermore, the third section will be about deep reinforcement learning, here we gathered deep reinforcement learning techniques. In the fourth section, we will summarize the reinforcement and deep reinforcement learning algorithms in detail. Furthermore, we will finalize the article with a discussion and a conclusion.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/TMVL9FIT/Moussaoui et al. - 2023 - Reinforcement Learning A review.pdf}
}

@article{poddar2023,
  title = {From {{Crowd Motion Prediction}} to {{Robot Navigation}} in {{Crowds}}},
  author = {Poddar, Sriyash and Mavrogiannis, Christoforos and Srinivasa, Siddhartha S.},
  year = {2023},
  month = oct,
  journal = {2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages = {6765--6772},
  publisher = {{IEEE}},
  address = {{Detroit, MI, USA}},
  doi = {10.1109/IROS55552.2023.10341464},
  urldate = {2024-01-20},
  abstract = {We focus on robot navigation in crowded environments. To navigate safely and efficiently within crowds, robots need models for crowd motion prediction. Building such models is hard due to the high dimensionality of multiagent domains and the challenge of collecting or simulating interaction-rich crowd-robot demonstrations. While there has been important progress on models for offline pedestrian motion forecasting, transferring their performance on real robots is nontrivial due to close interaction settings and novelty effects on users. In this paper, we investigate the utility of a recent state-of-the-art motion prediction model (S-GAN) for crowd navigation tasks. We incorporate this model into a model predictive controller (MPC) and deploy it on a self-balancing robot which we subject to a diverse range of crowd behaviors in the lab. We demonstrate that while S-GAN motion prediction accuracy transfers to the real world, its value is not reflected on navigation performance, measured with respect to safety and efficiency; in fact, the MPC performs indistinguishably even when using a simple constant-velocity prediction model, suggesting that substantial model improvements might be needed to yield significant gains for crowd navigation tasks. Footage from our experiments can be found at https://youtu.be/mzFiXgSKsZ0.},
  isbn = {9781665491907},
  file = {/Users/jonas/Zotero/storage/MR8SZCQZ/Poddar et al. - 2023 - From Crowd Motion Prediction to Robot Navigation i.pdf}
}

@inproceedings{portelas2020,
  title = {Automatic {{Curriculum Learning For Deep RL}}: {{A Short Survey}}},
  shorttitle = {Automatic {{Curriculum Learning For Deep RL}}},
  booktitle = {Proceedings of the {{Twenty-Ninth International Joint Conference}} on {{Artificial Intelligence}}},
  author = {Portelas, R{\'e}my and Colas, C{\'e}dric and Weng, Lilian and Hofmann, Katja and Oudeyer, Pierre-Yves},
  year = {2020},
  month = jul,
  pages = {4819--4825},
  publisher = {{International Joint Conferences on Artificial Intelligence Organization}},
  address = {{Yokohama, Japan}},
  doi = {10.24963/ijcai.2020/671},
  urldate = {2024-01-17},
  abstract = {Automatic Curriculum Learning (ACL) has become a cornerstone of recent successes in Deep Reinforcement Learning (DRL). These methods shape the learning trajectories of agents by challenging them with tasks adapted to their capacities. In recent years, they have been used to improve sample efficiency and asymptotic performance, to organize exploration, to encourage generalization or to solve sparse reward problems, among others. To do so, ACL mechanisms can act on many aspects of learning problems. They can optimize domain randomization for Sim2Real transfer, organize task presentations in multi-task robotic settings, order sequences of opponents in multi-agent scenarios, etc. The ambition of this work is dual: 1) to present a compact and accessible introduction to the Automatic Curriculum Learning literature and 2) to draw a bigger picture of the current state of the art in ACL to encourage the cross-breeding of existing concepts and the emergence of new ideas.},
  isbn = {978-0-9992411-6-5},
  langid = {english},
  keywords = {curriculum learning,reinforcement learning,survey/review/meta},
  file = {/Users/jonas/Zotero/storage/6W743GNM/Portelas et al. - 2020 - Automatic Curriculum Learning For Deep RL A Short.pdf}
}

@article{prakash2022,
  title = {Behavior-{{Aware Robot Navigation}} with {{Deep Reinforcement Learning}}},
  author = {Prakash, Varun Ganjigunte},
  year = {2022},
  month = dec,
  journal = {2022 6th International Conference on Computation System and Information Technology for Sustainable Solutions (CSITSS)},
  pages = {1--8},
  publisher = {{IEEE}},
  address = {{Bangalore, India}},
  doi = {10.1109/CSITSS57437.2022.10026363},
  urldate = {2024-01-20},
  abstract = {Robot navigation in a crowded area is difficult owing to the complexity of the natural world and the difficulties of simulating human interactions. In order for robots to safely and efficiently navigate near humans, it is imperative to include human behavior in robot navigation models. The prior techniques, on the other hand, isolate human behavior from models of robot navigation and solve robot navigation in a context-aware and partially observable setting. To us, this is the most rudimentary type of self-awareness that mobile robots may exhibit. We propose a risk-aware robot navigation model based on deep reinforcement learning that is capable of making improved navigational decisions. Our model provides safety, risk-awareness, and environmental understanding for mobile robots in both outdoor and indoor settings. We demonstrate our model's performance in both simulation and real-world robot environments.},
  isbn = {9781665456982 9781665456999}
}

@article{prudencio2023,
  title = {A {{Survey}} on {{Offline Reinforcement Learning}}: {{Taxonomy}}, {{Review}}, and {{Open Problems}}},
  shorttitle = {A {{Survey}} on {{Offline Reinforcement Learning}}},
  author = {Prudencio, Rafael Figueiredo and Maximo, Marcos R. O. A. and Colombini, Esther Luna},
  year = {2023},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  pages = {1--0},
  issn = {2162-2388},
  doi = {10.1109/TNNLS.2023.3250269},
  urldate = {2024-01-20},
  abstract = {With the widespread adoption of deep learning, reinforcement learning (RL) has experienced a dramatic increase in popularity, scaling to previously intractable problems, such as playing complex games from pixel observations, sustaining conversations with humans, and controlling robotic agents. However, there is still a wide range of domains inaccessible to RL due to the high cost and danger of interacting with the environment. Offline RL is a paradigm that learns exclusively from static datasets of previously collected interactions, making it feasible to extract policies from large and diverse training datasets. Effective offline RL algorithms have a much wider range of applications than online RL, being particularly appealing for real-world applications, such as education, healthcare, and robotics. In this work, we contribute with a unifying taxonomy to classify offline RL methods. Furthermore, we provide a comprehensive review of the latest algorithmic breakthroughs in the field using a unified notation as well as a review of existing benchmarks' properties and shortcomings. Additionally, we provide a figure that summarizes the performance of each method and class of methods on different dataset properties, equipping researchers with the tools to decide which type of algorithm is best suited for the problem at hand and identify which classes of algorithms look the most promising. Finally, we provide our perspective on open problems and propose future research directions for this rapidly growing field.},
  file = {/Users/jonas/Zotero/storage/96V6HYIF/Prudencio et al. - 2023 - A Survey on Offline Reinforcement Learning Taxono.pdf}
}

@misc{rail2024,
  title = {Making {{Real-World Reinforcement Learning Practical}}},
  author = {RAIL},
  year = {2024},
  month = jan,
  urldate = {2024-01-05},
  keywords = {reinforcement learning,robot}
}

@book{rokah2023,
  title = {Machine Learning for Data Science Handbook: Data Mining and Knowledge Discovery Handbook},
  shorttitle = {Machine Learning for Data Science Handbook},
  author = {Ro{\d k}a{\d h}, Liʾor and Maimon, Oded Z. and Shmueli, Erez},
  year = {2023},
  edition = {Third edition},
  publisher = {{Springer}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-24628-9},
  urldate = {2024-01-20},
  abstract = {Introduction to Knowledge Discovery and Data Mining -- Preprocessing Methods -- Data Cleansing: A Prelude to Knowledge Discovery -- Handling Missing Attribute Values -- Geometric Methods for Feature Extraction and Dimensional Reduction - A Guided Tour -- Dimension Reduction and Feature Selection -- Discretization Methods -- Outlier Detection -- Supervised Methods -- Supervised Learning -- Classification Trees -- Bayesian Networks -- Data Mining within a Regression Framework -- Support Vector Machines -- Rule Induction -- Unsupervised Methods -- A survey of Clustering Algorithms -- Association Rules -- Frequent Set Mining -- Constraint-based Data Mining -- Link Analysis -- Soft Computing Methods -- A Review of Evolutionary Algorithms for Data Mining -- A Review of Reinforcement Learning Methods -- Neural Networks For Data Mining -- Granular Computing and Rough Sets - An Incremental Development -- Pattern Clustering Using a Swarm Intelligence Approach -- Using Fuzzy Logic in Data Mining -- Supporting Methods -- Statistical Methods for Data Mining -- Logics for Data Mining -- Wavelet Methods in Data Mining -- Fractal Mining - Self Similarity-based Clustering and its Applications -- Visual Analysis of Sequences Using Fractal Geometry -- Interestingness Measures - On Determining What Is Interesting -- Quality Assessment Approaches in Data Mining -- Data Mining Model Comparison -- Data Mining Query Languages -- Advanced Methods -- Mining Multi-label Data -- Privacy in Data Mining -- Meta-Learning - Concepts and Techniques -- Bias vs Variance Decomposition for Regression and Classification -- Mining with Rare Cases -- Data Stream Mining -- Mining Concept-Drifting Data Streams -- Mining High-Dimensional Data -- Text Mining and Information Extraction -- Spatial Data Mining -- Spatio-temporal clustering -- Data Mining for Imbalanced Datasets: An Overview -- Relational Data Mining -- Web Mining -- A Review of Web Document Clustering Approaches -- Causal Discovery -- Ensemble Methods in Supervised Learning -- Data Mining using Decomposition Methods -- Information Fusion - Methods and Aggregation Operators -- Parallel and Grid-Based Data Mining {\textendash} Algorithms, Models and Systems for High-Performance KDD -- Collaborative Data Mining -- Organizational Data Mining -- Mining Time Series Data -- Applications -- Multimedia Data Mining -- Data Mining in Medicine -- Learning Information Patterns in Biological Databases - Stochastic Data Mining -- Data Mining for Financial Applications -- Data Mining for Intrusion Detection -- Data Mining for CRM -- Data Mining for Target Marketing -- NHECD - Nano Health and Environmental Commented Database -- Software -- Commercial Data Mining Software -- Weka-A Machine Learning Workbench for Data Mining., This book is a major update to the very successful first and second editions (2005 and 2010) of Data Mining and Knowledge Discovery Handbook. Since the last edition, this field has continued to evolve and to gain popularity. Existing methods are constantly being improved and new methods, applications and aspects are introduced. The new title of this handbook and its content reflect these changes thoroughly. Some existing chapters have been brought up to date. In addition to major revision of the existing chapters, the new edition includes totally new topics, such as: deep learning, explainable AI, human factors and social issues and advanced methods for big-data. The significant enhancement to the content reflects the growth in importance of data science. The third edition is also a timely opportunity to incorporate many other changes based on peers and students' feedback. This comprehensive handbook also presents a coherent and unified repository of data science major concepts, theories, methods, trends, challenges and applications. It covers all the crucial important machine learning methods used in data science. Today's accessibility and abundance of data make data science matters of considerable importance and necessity. Given the field's recent growth, it's not surprising that researchers and practitioners now have a wide range of methods and tools at their disposal. While statistics is fundamental for data science, methods originated from artificial intelligence, particularly machine learning, are also playing a significant role. This handbook aims to serve as the main reference for researchers in the fields of information technology, e-Commerce, information retrieval, data science, machine learning, data mining, databases and statistics as well as advanced level students studying computer science or electrical engineering. Practitioners working within these related fields and data scientists will also want to purchase this handbook as a reference.},
  isbn = {978-3-031-24627-2},
  langid = {english},
  lccn = {ST 300 R742(3)}
}

@article{samsani2021,
  title = {Socially {{Compliant Robot Navigation}} in {{Crowded Environment}} by {{Human Behavior Resemblance Using Deep Reinforcement Learning}}},
  author = {Samsani, Sunil Srivatsav and Muhammad, Mannan Saeed},
  year = {2021},
  month = jul,
  journal = {IEEE Robotics and Automation Letters},
  volume = {6},
  number = {3},
  pages = {5223--5230},
  issn = {2377-3766, 2377-3774},
  doi = {10.1109/LRA.2021.3071954},
  urldate = {2024-01-20},
  abstract = {Social robots have evolved in diverse applications with the emergence of deep reinforcement learning methods. However, safe and secure navigation of social robots in a complex crowded environment remains a challenging task. The robot can safely navigate in a crowd only if it can predict the next action of humans, however this task becomes difficult because of the unpredictable human behavior. To address the issue of socially compliant navigation, the robot needs to learn real-time human behavior. This manuscript models Danger-Zone for the robot by considering all possible actions that humans can take at given time. The Danger Zones are formulated by considering the real time human behavior. The robot is trained to avoid these danger zones for safe and secure navigation. The proposed model is tested on the three state of art methods, Collision Avoidance with Deep Reinforcement Learning (CADRL), Long Short Term Memory Reinforcement Learning (LSTM-RL) and Social Attention with Reinforcement Learning (SARL) in multi-agent navigation. Experimental results signify that proposed model can understand human behavior and navigate in a socially compliant manner with safety as the highest priority.}
}

@article{schmidhuber2015a,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  shorttitle = {Deep {{Learning}} in {{Neural Networks}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = jan,
  journal = {Neural Networks},
  volume = {61},
  eprint = {1404.7828},
  primaryclass = {cs},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  urldate = {2024-01-17},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archiveprefix = {arxiv},
  file = {/Users/jonas/Zotero/storage/9TX8QIMG/Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf;/Users/jonas/Zotero/storage/8GDUABXC/1404.html}
}

@article{sun2021,
  title = {Motion {{Planning}} for {{Mobile Robots}}{\textemdash}{{Focusing}} on {{Deep Reinforcement Learning}}: {{A Systematic Review}}},
  shorttitle = {Motion {{Planning}} for {{Mobile Robots}}{\textemdash}{{Focusing}} on {{Deep Reinforcement Learning}}},
  author = {Sun, Huihui and Zhang, Weijie and Yu, Runxiang and Zhang, Yujie},
  year = {2021},
  journal = {IEEE Access},
  volume = {9},
  pages = {69061--69081},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2021.3076530},
  urldate = {2024-01-20},
  abstract = {Mobile robots contributed significantly to the intelligent development of human society, and the motion-planning policy is critical for mobile robots. This paper reviews the methods based on motion-planning policy, especially the ones involving Deep Reinforcement Learning (DRL) in the unstructured environment. The conventional methods of DRL are categorized to value-based, policy-based and actor-critic-based algorithms, and the corresponding theories and applications are surveyed. Furthermore, the recently-emerged methods of DRL are also surveyed, especially the ones involving the imitation learning, meta-learning and multi-robot systems. According to the surveys, the potential research directions of motion-planning algorithms serving for mobile robots are enlightened.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/RYS39KUF/Sun et al. - 2021 - Motion Planning for Mobile Robots—Focusing on Deep.pdf}
}

@inproceedings{trautman2020,
  title = {Real {{Time Crowd Navigation}} from {{First Principles}} of {{Probability Theory}}},
  booktitle = {Proceedings of the {{International Conference}} on {{Automated Planning}} and {{Scheduling}}},
  author = {Trautman, Peter and Patel, Karankumar},
  year = {2020},
  month = jun,
  volume = {30},
  pages = {459--467},
  issn = {2334-0843, 2334-0835},
  doi = {10.1609/icaps.v30i1.6741},
  urldate = {2024-01-20},
  abstract = {Constructing realistic and real time human-robot interaction models is a core challenge in crowd navigation. In this paper we derive a robot-agent interaction density from first principles of probability theory; we call our approach ``first order interacting Gaussian processes'' (foIGP). Furthermore, we compute locally optimal solutions{\textemdash}with respect to multi-faceted agent ``intent'' and ``flexibility''{\textemdash}in near real time on a laptop CPU. We test on challenging scenarios from the ETH crowd dataset and show that the safety and efficiency statistics of foIGP is competitive with human safety and efficiency statistics. Further, we compute the safety and efficiency statistics of dynamic window avoidance, a physics based model variant of foIGP, a Monte Carlo inference based approach, and the best performing deep reinforcement learning algorithm; foIGP outperforms all of them.},
  file = {/Users/jonas/Zotero/storage/KEL3LWN9/Trautman und Patel - 2020 - Real Time Crowd Navigation from First Principles o.pdf}
}

@inproceedings{wang2022,
  title = {Metrics for {{Evaluating Social Conformity}} of {{Crowd Navigation Algorithms}}},
  booktitle = {2022 {{IEEE International Conference}} on {{Advanced Robotics}} and {{Its Social Impacts}} ({{ARSO}})},
  author = {Wang, Junxian and Chan, Wesley P. and {Carreno-Medrano}, Pamela and Cosgun, Akansel and Croft, Elizabeth},
  year = {2022},
  month = may,
  pages = {1--6},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/ARSO54254.2022.9802981},
  urldate = {2024-01-10},
  abstract = {Recent protocols and metrics for training and evaluating autonomous robot navigation through crowds are inconsistent due to diversified definitions of ``social behavior''. This makes it difficult, if not impossible, to effectively compare published navigation algorithms. Without a good evaluation protocol resulting algorithms may fail to generalize, due to lack of diversity in training. To address these gaps, this paper facilitates a more comprehensive evaluation and objective comparison of crowd navigation algorithms by proposing a consistent set of metrics that accounts for both efficiency and social conformity. The paper also provides a systematic protocol comprising multiple crowd navigation scenarios of varying complexity for evaluation. We tested four state-of-theart algorithms under this protocol. Results revealed that some state-of-the-art algorithms have great challenges in generalizing, and using our protocol for training, we were able to improve the algorithm's performance. We demonstrate that the set of proposed metrics provides more insight and effectively differentiates the performance of these algorithms with respect to efficiency and social conformity.},
  isbn = {978-1-66547-966-0},
  langid = {english},
  keywords = {robot},
  file = {/Users/jonas/Zotero/storage/7A4IIPQN/Wang et al. - 2022 - Metrics for Evaluating Social Conformity of Crowd .pdf}
}

@article{wang2022a,
  title = {Feedback-Efficient {{Active Preference Learning}} for {{Socially Aware Robot Navigation}}},
  author = {Wang, Ruiqi and Wang, Weizheng and Min, Byung-Cheol},
  year = {2022},
  month = oct,
  journal = {2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages = {11336--11343},
  publisher = {{IEEE}},
  address = {{Kyoto, Japan}},
  doi = {10.1109/IROS47612.2022.9981616},
  urldate = {2024-01-20},
  abstract = {Socially aware robot navigation, where a robot is required to optimize its trajectory to maintain comfortable and compliant spatial interactions with humans in addition to reaching its goal without collisions, is a fundamental yet challenging task in the context of human-robot interaction. While existing learning-based methods have achieved better performance than the preceding model-based ones, they still have drawbacks: reinforcement learning depends on the handcrafted reward that is unlikely to effectively quantify broad social compliance, and can lead to reward exploitation problems; meanwhile, inverse rein-forcement learning suffers from the need for expensive human demonstrations. In this paper, we propose a feedback-efficient active preference learning approach, FAPL, that distills human comfort and expectation into a reward model to guide the robot agent to explore latent aspects of social compliance. We further introduce hybrid experience learning to improve the efficiency of human feedback and samples, and evaluate benefits of robot behaviors learned from FAPL through extensive simulation experiments and a user study (N=10) employing a physical robot to navigate with human subjects in real-world scenarios. Source code and experiment videos for this work are available at: https://sites.google.com/view/san-fapl.},
  isbn = {9781665479271},
  file = {/Users/jonas/Zotero/storage/ZW5LWAYI/Wang et al. - 2022 - Feedback-efficient Active Preference Learning for .pdf}
}

@article{wang2023,
  title = {{{NaviSTAR}}: {{Socially Aware Robot Navigation}} with {{Hybrid Spatio-Temporal Graph Transformer}} and {{Preference Learning}}},
  shorttitle = {{{NaviSTAR}}},
  author = {Wang, Weizheng and Wang, Ruiqi and Mao, Le and Min, Byung-Cheol},
  year = {2023},
  month = oct,
  journal = {2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages = {11348--11355},
  publisher = {{IEEE}},
  address = {{Detroit, MI, USA}},
  doi = {10.1109/IROS55552.2023.10341395},
  urldate = {2024-01-20},
  abstract = {Developing robotic technologies for use in human society requires ensuring the safety of robots' navigation behaviors while adhering to pedestrians' expectations and social norms. However, understanding complex human-robot interactions (HRI) to infer potential cooperation and response among robots and pedestrians for cooperative collision avoid-ance is challenging. To address these challenges, we propose a novel socially-aware navigation benchmark called NaviS Tar, which utilizes a hybrid Spatio- Temporal grAph tRansformer to understand interactions in human-rich environments fusing crowd multi-modal dynamic features. We leverage an off-policy reinforcement learning algorithm with preference learning to train a policy and a reward function network with supervi-sor guidance. Additionally, we design a social score function to evaluate the overall performance of social navigation. To compare, we train and test our algorithm with other state-of-the-art methods in both simulator and real-world scenarios independently. Our results show that NaviSTAR outperforms previous methods with outstanding performance11The source code and experiment videos of this work are available at: https://sites.google.com/view/san-navistar},
  isbn = {9781665491907},
  file = {/Users/jonas/Zotero/storage/8C6XW3XQ/Wang et al. - 2023 - NaviSTAR Socially Aware Robot Navigation with Hyb.pdf}
}

@article{zeng2021,
  title = {Robot {{Navigation}} in {{Crowd Based}} on {{Dual Social Attention Deep Reinforcement Learning}}},
  author = {Zeng, Hui and Hu, Rong and Huang, Xiaohui and Peng, Zhiying},
  editor = {Velasco Villa, Martin},
  year = {2021},
  month = sep,
  journal = {Mathematical Problems in Engineering},
  volume = {2021},
  pages = {1--11},
  issn = {1563-5147, 1024-123X},
  doi = {10.1155/2021/7114981},
  urldate = {2024-01-20},
  abstract = {Finding a feasible, collision-free path in line with social activities is an important and challenging task for robots working in dense crowds. In recent years, many studies have used deep reinforcement learning techniques to solve this problem. In particular, it is necessary to find an efficient path in a short time which often requires predicting the interaction with neighboring agents. However, as the crowd grows and the scene becomes more and more complex, researchers usually simplify the problem to a one-way human-robot interaction problem. But, in fact, we have to consider not only the interaction between humans and robots but also the influence of human-human interactions on the movement trajectory of the robot. Therefore, this article proposes a method based on deep reinforcement learning to enable the robot to avoid obstacles in the crowd and navigate smoothly from the starting point to the target point. We use a dual social attention mechanism to jointly model human-robot and human-human interaction. All sorts of experiments demonstrate that our model can make robots navigate in dense crowds more efficiently compared with other algorithms.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/HAI3TQBF/Zeng et al. - 2021 - Robot Navigation in Crowd Based on Dual Social Att.pdf}
}

@article{zhang2023,
  title = {Crowd-{{Aware Mobile Robot Navigation Based}} on {{Improved Decentralized Structured RNN}} via {{Deep Reinforcement Learning}}},
  author = {Zhang, Yulin and Feng, Zhengyong},
  year = {2023},
  month = feb,
  journal = {Sensors},
  volume = {23},
  number = {4},
  pages = {1810},
  issn = {1424-8220},
  doi = {10.3390/s23041810},
  urldate = {2024-01-20},
  abstract = {Efficient navigation in a socially compliant manner is an important and challenging task for robots working in dynamic dense crowd environments. With the development of artificial intelligence, deep reinforcement learning techniques have been widely used in the robot navigation. Previous model-free reinforcement learning methods only considered the interactions between robot and humans, not the interactions between humans and humans. To improve this, we propose a decentralized structured RNN network with coarse-grained local maps (LM-SRNN). It is capable of modeling not only Robot{\textendash}Human interactions through spatio-temporal graphs, but also Human{\textendash}Human interactions through coarse-grained local maps. Our model captures current crowd interactions and also records past interactions, which enables robots to plan safer paths. Experimental results show that our model is able to navigate efficiently in dense crowd environments, outperforming state-of-the-art methods.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/KPPB35IT/Zhang und Feng - 2023 - Crowd-Aware Mobile Robot Navigation Based on Impro.pdf}
}

@article{zhou2022,
  title = {Robot Navigation in a Crowd by Integrating Deep Reinforcement Learning and Online Planning},
  author = {Zhou, Zhiqian and Zhu, Pengming and Zeng, Zhiwen and Xiao, Junhao and Lu, Huimin and Zhou, Zongtan},
  year = {2022},
  month = oct,
  journal = {Applied Intelligence},
  volume = {52},
  number = {13},
  pages = {15600--15616},
  issn = {0924-669X, 1573-7497},
  doi = {10.1007/s10489-022-03191-2},
  urldate = {2024-01-20},
  abstract = {Navigating mobile robots along time-efficient and collision-free paths in crowds is still an open and challenging problem. The key is to build a profound understanding of the crowd for mobile robots, which is the basis of a proactive and foresighted policy. However, since the interaction mechanisms among pedestrians are complex and sophisticated, it is difficult to describe and model them accurately. For the excellent approximation capability of deep neural networks, deep reinforcement learning is a promising solution to the problem. However, current model-free learning-based approaches in crowd navigation always neglect planning and still lead to reactive collision avoidance policies and shortsighted behaviors. Meanwhile, most model-based learning-based approaches are based on state values, imposing a substantial computational burden. To address these problems, we propose a graph-based deep reinforcement learning method, social graph-based double dueling deep Q-network (SG-D3QN), that (i) introduces a social attention mechanism to extract an efficient graph representation for the crowd-robot state, (ii) extends the previous state value approximator to a state-action value approximator, (iii) further optimizes the state-action value approximator with simulated experiences generated by the learned environment model, and (iv) then proposes a human-like decision-making process by integrating model-free reinforcement learning and online planning. Experimental results indicate that our approach helps the robot understand the crowd and achieves a high success rate of more than 0.99 in the crowd navigation task. Compared with previous state-of-the-art algorithms, our approach achieves better performance. Furthermore, with the human-like decision-making process, our approach incurs less than half of the computational cost.},
  langid = {english},
  file = {/Users/jonas/Zotero/storage/NMQYD8D3/Zhou et al. - 2022 - Robot navigation in a crowd by integrating deep re.pdf}
}
